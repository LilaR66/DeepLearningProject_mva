# Deep Learning Project for MVA

## Paper Reference
The project is based on the paper:  
**[An Image is Worth 16X16 Words: Transformers For Image Recognition at Scale](https://arxiv.org/pdf/2010.11929)**

In this project, we reimplemented our own Vision Transformer (ViT) from scratch, improving upon the implementation found in:  
**[Implementing Vision Transformer (ViT) from Scratch](https://towardsdatascience.com/implementing-vision-transformer-vit-from-scratch-3e192c6155f0)**

## Authors
- **Lila Roig**
- **Romane Barra**

## Professor
- **V. Lepetit**

## Project Structure

- **biblio**: Papers on which the project is based.
- **code**:
    - **experiments**: Directory containing saved models.
    - **LilaRoig_RomaneBarraDeepLearningProject.ipynb**: Jupyter notebook containing the full code for the project.
- **poster**: Project poster.

## Description

This project aims to implement the Vision Transformer (ViT) from scratch, which revolutionized image recognition tasks by treating images as sequences of patches, similar to how text is processed by Transformers. We introduced several improvements and modifications to the original ViT implementation, optimizing it for better performance in large-scale image recognition tasks.
